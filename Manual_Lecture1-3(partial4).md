# CS6493 NLP

## Lecture 1 Introduction

### What is Natural Language Processing?

* A branch of Artificial Intelligence.

* Make computers to learn, process and manipulate natural languages to interact with humans.

### Birdâ€™s-eye view of this course

* Basics: Linguistics, language models, word embeddings

* Tasks: NLU, NLG, machine translation, question answering, dialogue, text
  classification

* Large language models: Transformers, pretraining (e.g., BERT, GPT),
  prompting and alignment, LLM agent, efficient finetuning, RAG

### Why is NLP challenging?

- Ambiguity:
  
  - Similar strings mean different things, different strings mean the same thing.

- Context(ä¸Šä¸‹æ–‡ä¸åŒï¼Œwordå«ä¹‰ä¸åŒ)

- Commonsense knowledgeï¼ˆæ ¹æ®å¸¸è¯†çŸ¥è¯†æ‰€å¯¼è‡´çš„ä¸åŒç»“æœï¼Œæ¯”æ–¹è¯´ï¼Œæ²™å‘æ¬ä¸è¿›å»ï¼Œå› ä¸ºå¤ªå®½äº†(æ²™å‘)/å¤ªçª„äº†(é—¨æ¡†)ï¼‰

### Preprocessing of data(NLP first step)

* Tokenization
  
  * Tokenization is the process of breaking up text document into individual words called tokens. 
  
  * Tokens can be either words, characters, or sub-words (oov, BPE).

* Stop words removal 

* Stemming(reducing a word to its stem/root word,ä½†æ¯”è¾ƒç²—ç³™ï¼Œå¯èƒ½éš¾ä»¥ä¿ç•™æœ‰æ•ˆå•è¯ï¼Œæ¯”æ–¹è¯´â€œtroubleâ€, â€œtroubledâ€ and â€œtroublesâ€ might actually be converted to troubl)

* Lemmatization(å’Œstemmingç±»ä¼¼ï¼Œä½†ä¼šä¿ç•™æœ‰æ•ˆå•è¯ï¼Œå¦‚ä¸Šä¾‹ï¼Œä½†æ˜¯ä¼šæ ¹æ®è¯æ³•è¿˜åŸæˆtrouble)ï¼Œå¸¸è§è°ƒç”¨åº“ä¸ºWordNetï¼ŒParts of Speech(PoS) tagging(ç”¨äºåˆ†ç±»æ‰“æ ‡ç­¾ï¼Œä¸å¤ªç¡®å®šè¿™ä¸ªè¯¥åˆ†åˆ°å“ªé‡Œ)

* Vectorization (N-gram, BOW, TF-IDF)
  
  * N-grams: å¯¹ä¸€ä¸ªæœ‰Xä¸ªwordsçš„å¥å­æ¥è¯´ï¼Œä¼šæœ‰X-N+1ä¸ªgrams
  
  * Bag of words
    
    * Intuition: two sentences are similar if they contain similar set of
      words.
    
    * å°†æ–‡æ¡£æ¶‰åŠåˆ°çš„wordåˆ‡å¼€æ¥ç„¶åæ„å»ºä¸€ä¸ªè¯æ±‡è¡¨ï¼Œéšåè¿›è¡ŒOne-hot
    
    * ç¼ºç‚¹ï¼šVector length = vocabulary size. Sparse metric with many 0s. Retain no grammar or ordering information.
  
  * TF-IDF
    
    * each text sentence is called a **document**ï¼Œcollection of such documents is referred to as **text corpus**.
    
    * reflect how important a word is to a document in a collection or corpus
    
    * Term Frequency: è¡¡é‡ term tå‡ºç°åœ¨æ–‡æ¡£dçš„é¢‘ç‡ TF=num t / sum of num of words in d
    
    * Inverse Document Frequency: IDF=log(æ€»æ–‡æ¡£æ•°/æœ‰tå‡ºç°çš„æ–‡æ¡£æ•°)
    
    * TF-IDF=TF*IDF

* Tokenization Methods
  
  * Word Tokenization,æµ‹è¯•æ—¶å¯èƒ½ä¼šé‡åˆ°è¯æ±‡è¡¨ä¸å­˜åœ¨çš„æ–°è¯ï¼Œè§£å†³ï¼šé€‰æ‹©è®­ç»ƒæ•°æ®ä¸­**å‰Kä¸ªæœ€é¢‘ç¹çš„å•è¯**ç»„æˆè¯æ±‡è¡¨ã€‚å°†è®­ç»ƒæ•°æ®ä¸­çš„ç¨€æœ‰è¯æ›¿æ¢ä¸º**æœªçŸ¥è¯å…ƒï¼ˆUNKï¼‰**ã€‚ä½†é—®é¢˜å¯èƒ½ä¼šæ¿€å¢
  
  * Character Tokenizationï¼Œè¾“å…¥å¾ˆå¤§è¾“å‡ºå¾ˆå¤§
  
  * Subword Tokenization(â€œlowerâ€ åˆ†å‰²ä¸º â€œlow-erâ€ã€‚)
  
  * BPE ,åº”ç”¨äºTransformer
    
    1. å°†è¯­æ–™åº“ä¸­çš„å•è¯åˆ†å‰²æˆå­—ç¬¦ï¼Œå¹¶åœ¨æ¯ä¸ªå•è¯æœ«å°¾æ·»åŠ </w>æ ‡è®°ã€‚
    
    2. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ŒåŒ…å«è¯­æ–™åº“ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ã€‚
    
    3. è®¡ç®—è¯­æ–™åº“ä¸­å­—ç¬¦æˆ–å­—ç¬¦åºåˆ—å¯¹çš„é¢‘ç‡ã€‚
    
    4. å°†æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹åˆå¹¶ã€‚
    
    5. å°†åˆå¹¶åçš„æœ€ä½³å­—ç¬¦å¯¹ä¿å­˜åˆ°è¯æ±‡è¡¨ã€‚
    
    6. é‡å¤æ­¥éª¤3åˆ°5ï¼Œè¿­ä»£è‹¥å¹²æ¬¡ã€‚

![](.\img\1-1-BPE.png)

## Lecture 2 Language Models

### Language model definition & applications

* infinite monkey theorem: å‡è®¾ ä¸€åªçŒ´å­ä¸æ–­åœ°æ•²é”®ç›˜ï¼Œæ€»æœ‰å¯èƒ½æ•²å‡ºæ¥æœ‰æ„ä¹‰çš„è¯

* language model: **probability distribution** over sequences of words. length T,it assigns a probability ğ‘ƒ(ğ‘¥(1), ğ‘¥(2), â€¦ ğ‘¥(ğ‘‡)) to the wholesequence. 

* The task of language models could be regarded as **assigning probability to a piece of text**.![](./img/2-1-1-LMDefinition.png)

* It serves as a **benchmark** task that helps us to measure our progress on understanding language

* It is a **subcomponant** of many NLP tasks, especially those involving generating text and estimating the probability of the text

* LMçš„ç›®çš„æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥æˆ‘ä»¬è€ƒè™‘ç»™ä¸€ç³»åˆ—è¯x1,x2,...,xt,ç„¶åè®¡ç®—xt+1çš„æ¦‚ç‡åˆ†å¸ƒp(xt+1|x1...xt)

* é©¬å°”ç§‘å¤«å‡è®¾äº†xt+1ä¾èµ–äºå‰n-1ä¸ªå•è¯ï¼Œæ‰€ä»¥æœ‰æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„æ¡ä»¶æ¦‚ç‡ï¼Œè¯¥æ¡ä»¶æ¦‚ç‡=P(n-gram)/P((n-1 )gram),å¯ä»¥è¿‘ä¼¼åœ°ç”¨é¢‘ç‡è¿‘ä¼¼æ¦‚ç‡

### Model construction

#### Statistical language models

* n-gram
  
  - Unigram:![](./img/2-1-2-Unigram.png)
  - Full history Model: ä¾èµ–äºä¹‹å‰æ•´ä¸ªå†å²
  - ![](./img/2-1-3-4gramModel.png)
  - ç¨€ç–æ€§é—®é¢˜ä¸­ï¼Œåˆ†å­ä¸­wå¯èƒ½ä¸å‡ºç°å¯¼è‡´æ¦‚ç‡ä¸º0ï¼Œåˆ†æ¯ä¸­è‹¥ä¸å‡ºç°åˆ™å¯¼è‡´æ— æ³•è®¡ç®—ï¼Œç„¶åè¿˜æœ‰å­˜å‚¨é—®é¢˜éœ€è¦å­˜å‚¨æ‰€æœ‰è¯­æ–™ä¸­çš„n-gramsï¼Œæ‰€ä»¥å¢å¤§è¯­æ–™ä¹Ÿä¼šå¢å¤§æ¨¡å‹å¤§å°
  - ![](./img/2-1-4-gramsol.png)
  - å¯¹äºåˆ†å­æˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸€ä¸ªå°çš„Î´æ¥åšä¸€ä¸ªå¹³æ»‘ï¼Œå¯¹äºåˆ†æ¯æˆ‘ä»¬å¯ä»¥è¿›è¡Œå›é€€åˆ°æ›´å°çš„åˆ†å‰²ã€‚

#### Neural language models

* **å›ºå®šçª—å£ç¥ç»ç½‘ç»œæ¨¡å‹**
  
  * ![](./img/2-2-1-base.png)
  
  * é¦–å…ˆæ˜¯å°†è¾“å…¥çš„å•è¯è¿›è¡Œäº†ç‹¬çƒ­ç¼–ç ï¼Œç„¶åè¿›è¡Œè¯åµŒå…¥çš„æ‹¼æ¥ï¼Œæ¥ç€è¾“å…¥åˆ°äº†éšè—å±‚è¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡softmaxå‡½æ•°è½¬åŒ–ä¸ºæ¦‚ç‡è¾“å‡º
  
  * æå‡å’Œä»å­˜åœ¨çš„é—®é¢˜è§å›¾å³

* **Recurrent Neural Network(RNN)**
  
  * Recurrent(å¾ªç¯) units: blocks share the structure and parameters (weights)
  
  * Flexible input / output size
  
  * Self-connections or connections to units in the previous layers
  
  * Short-term memory
  
  * ![](./img/2-2-2-RNNStructure.png)
  
  * ![](./img/2-2-3-RNNBlock.png)
  
  * RNNå—ï¼Œä¸ºå›¾å½“ä¸­ä¸¤ä¸ªå…¬å¼çš„å…·ä½“è¡¨è¾¾ï¼Œå¯ä»¥è·Ÿç€èµ°ä¸€éï¼Œé¦–å…ˆæˆ‘ä»¬å°†è¿™ä¸‰ä¸ªå‚æ•°æ±‚å’Œåç”¨g1å‡½æ•°è¿›è¡Œäº†å¤„ç†å¾—åˆ°äº†atï¼Œå¦‚æœå½“å‰éœ€è¦è¾“å‡ºï¼Œæˆ‘ä»¬åˆ™ç»§ç»­è¿›è¡Œytçš„è®¡ç®—ã€‚
  
  * ![](./img/2-2-4-RNNConcatenation.png)
  
  * è¿›è¡Œå¯¹æ¯”æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹å‡ºè¿™é‡Œçš„hæ˜¯éšè—å±‚çš„è¾“å‡º
  
  * è¿™é‡Œçš„tanhæ˜¯ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œè€Œæˆ‘ä»¬è¿™ä¸ªæ¿€æ´»å‡½æ•°å¯ä»¥å½“ä½œä¸€ä¸ªsoft switch(å¼€å…³)ï¼Œæ¥æ§åˆ¶ä¿¡å·æµçš„å¼ºåº¦ï¼ŒåŒæ—¶å¯ä»¥squash(å‹ç¼©)å€¼åˆ°å›ºå®šèŒƒå›´ï¼Œæ¯”å¦‚0ï¼Œ1
  
  * ![](./img/2-2-5-RNN.png)
  
  * æ­¤å¤„æœ€å¥½å¯¹æ¯”è®°å¿†ä¼˜ç‚¹å’Œä¹‹å‰çš„å›ºå®šçª—å£NN
  
  * RNNçš„lossä¹Ÿæ˜¯è€ƒè™‘äº†ä¸€ä¸ªäº¤å‰ç†µæŸå¤±ï¼Œç„¶åå–å¹³å‡ï¼Œä¸è¿‡è®¡ç®—æ•´ä¸ªcorpusçš„losså’Œgradient costå¾ˆé«˜ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œé‡‡ç”¨äº†SGD(Stochastic Gradient Descent)æ¥è¿›è¡Œå°å—çš„è¿›è¡Œæ›´æ–°losså’Œgradientã€‚
  
  * ![](./img/2-2-6-RNNLoss.png)
  
  * How to calculate the derivatives of ğ½(ğœƒ) with respect to the repeated weight matrix ğ‘Šâ„ through time ğ½ğ‘¡ (ğœƒ)? é€šè¿‡é“¾å¼æ³•åˆ™æ¥è®¡ç®—ï¼ŒåŒæ—¶è¿ç”¨äº†åå‘ä¼ æ’­
  
  * åœ¨RNNä¸­ï¼Œthe loss function of all time steps is defined based on the loss at every time step, e.g., cross entropy loss. ç„¶ååå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦æ›´æ–°could increase/decrease exponentially with respect to the number of layers, leading to **ineffective weight updates**. è¿™å°±å¯¼è‡´äº†**gradient vanishing/exploding**
  
  * ç”±äºå­˜åœ¨ç€è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æ˜¯unable to predict similar long-distance dependencies at test time

* **Long Short-Term Memory**
  
  * a special RNN to slove vanish gradient
    phenomenon
  
  * it uses a set of gating units to control the
    memory.
  
  * ![](./img/2-2-7-LSTM.png)
  
  * ![](./img/2-2-8-LSTMBlock.png)
  
  * LSTMæœ‰ç‚¹å¤æ‚ï¼Œç„¶ååˆæœ‰äº†ä¸€ä¸ªç®€åŒ–ç‰ˆçš„LSTMå«åšGRUã€‚
  
  * ![](./img/2-2-9-GRU.png)
  
  * LSTMæ˜¯å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œlong-termé•¿æœŸä¾èµ–çš„é—®é¢˜çš„ï¼ŸMaybe because of its gated design.
  
  * The LSTM architecture makes it easier for the RNN to preserve information over many timesteps
    â— e.g., if the forget gate is set to 1 for a cell dimension and the input gate set to
    0, then the information of that cell is preserved indefinitely.
    â— By contrast, itâ€™s harder for traditional RNNs to learn a recurrent weight matrix
    ğ‘Šâ„ that preserves info in hidden state.

### Language model evaluation

* **Perplexity**

* ![](./img/2-3-1-perplexity.png)
  
  * LSTM < RNN < n-gram

* Applying:
  
  * ![](./img/2-3-2-ML.png)
  * ![](./img/2-3-3-NER.png)
  * ![](./img/2-3-5-QA.png)
  * ![](./img/2-3-4-SR.png)

## Lecture 3 Word Embedding

### Word embedding definition and principles

* Meaning of words in human languages
  
  * the thing one intends to convey especially by language(è¯´è¯çš„äººæƒ³è¯´çš„)
  
  * the thing that is conveyed by language(æ¥æ”¶è€…æ‰€æ¥æ”¶åˆ°çš„)
  
  * Denotational Semantics(è¯­ä¹‰å­¦): Signifier(ç¬¦å·æœ¬èº«ï¼Œè­¬å¦‚å•è¯çš„æ‹¼å†™æˆ–è€…å‘éŸ³)<=> Signified(ç¬¦å·ä»£è¡¨çš„å…·ä½“äº‹åŠ¡æˆ–è€…æŠ½è±¡æ¦‚å¿µ)

* **WordNet**(a lexical database of semantic relations between words in more than 200 languages): synonyms and hypernyms(åŒä¹‰è¯å’Œè¿‘ä¹‰è¯)
  
  * **Problems**:
    
    * Missing **nuance**(æ„ä¹‰). Is â€œproficientâ€ always a synonym of â€œgoodâ€?
    
    * Impossible to keep up-to-date
    
    * Subjective ä¸»è§‚
    
    * **Human labor** for creation and adaptation
    
    * Cannot compute accurate word **similarity**

* **One-hot vecto**r: discrete symbols
  
  * A localist representation
  
  * One-hot vectors
    â—‹ one 1, the rest 0
  
  * Vector diemension = number of words in vocabulary
  
  * **Problems**: Thereâ€™s no natural notation for one-hot vectors!
  
  * **Solution** :Learn to **encode similarity**(Distributional hypothesis) in the vectors themselves.

* Distributional hypothesis:
  
  * Words that occur in **similar contexts** tend to have **similar meanings**
  
  * When a word w appears in a text, its **context** is the set of words that appear nearby (with a fixed-size window) Use many contexts of w to build up a representation of w

### Embedding methods â€“ word2vec

#### Continuous bag-of-words

* Word embeddings - goal
  
  * Build a dense vector for each word
  
  * A word vector should be **similar** to vectors of words that appear in similar contexts
  
  * Word embeddings also called word vectors or (neural) word representation
  
  * A distributed representation

* Word2vec
  
  * Word2vec (Mikolov et al. 2013) is a framework for learning word vectors.
  
  * Idea:
    
    * Input: Given a large corpus of text (e.g., a bunch of sentences or documents)
    
    * Output: Every word in a fixed vocabulary is represented by a vector
    
    * Go through each position t in the text, which has a target word (or center word) ğ‘¤ğ‘¡ and several context words ğ‘¤ğ‘
    
    * Use the **similarity** **of the word vectors** for ğ‘¤ğ‘¡ and ğ‘¤ğ‘
      â—‹ **Skip-gram**: to calculate the probability of context words ğ‘¤ğ‘ given the target word ğ‘¤ğ‘¡
      â—‹ **Continuous bag of words (CBOW)**: to calculate the probability of target word ğ‘¤ğ‘¡ given context words ğ‘¤ğ‘
    
    * **Keep adjusting the word vectors** to maximize the probability
  
  * ![](./img/3-1-SkipVSCBOW.png)

#### Skip-gram

* ![](./img/3-2-SkipExample.png)
* ![](./img/3-3-SkipOpt.png)
* ![](./img/3-4-SkipNN.png)
  * Hidden layer weight matrix = word vector lookup(æŸ¥æ‰¾)
  * Output layer weight matrix = weighted sum as final score. $S_j=hv_{w_j}'$ æœ€åçš„æ¦‚ç‡ç”¨softmaxæ¥è¾“å‡º
  * ![](./img/3-5-Softmax.png)
  * éšåç»™å‡ºäº†ç›®æ ‡æ­¤çš„æŸå¤±å‡½æ•°ï¼Œç„¶åå†™å‡ºæ¥åå‘ä¼ æ’­çš„å…¬å¼æ¨å¯¼ï¼Œéšåä½¿ç”¨SGDæ¥è¿›è¡Œæ›´æ–°

### Improve training efficiency

* Reason:
  
  * The size of vocabulary V is impressively large
  
  * Evaluation of the objective function would take O(V) time

* Solution:
  
  * è´Ÿé‡‡æ ·
  
  * å±‚æ¬¡softmax

* Comparison:
  
  * Hierarchical softmax tends to be better for **infrequent words**.
  
  * Negative sampling works better for **frequent words and lower dimensional vectors**

#### Negative sampling

* A simplified version of NCE (Noise Contrastive Estimation) 
  
  * Sample from a **noise distribution** ğ‘ƒğ‘›(ğ‘¤) 
  
  * The probabilities in ğ‘ƒğ‘›(ğ‘¤) match the ordering of the frequency in the vocabulary 
  
  * Pick out k words from ğ‘ƒğ‘›(ğ‘¤), training together with the center word
  
  * Convert to (k+1) **binary classification problems** 
  
  * e.g., in tensorflow, the probability distribution to select samples: (decreasing in s(w))

* Process:
  
  * Target word ğ‘¤ğ‘– and context word ğ‘¤ğ‘—
  
  * From ğ‘ƒğ‘›(ğ‘¤), based on a certain probability distribution, pick out k words ğ‘¤1, ğ‘¤2, â€¦ , ğ‘¤ğ‘˜
  
  * Positive sample: {ğ‘¤ğ‘–, ğ‘¤ğ‘—}
  
  * Negative samples: {ğ‘¤ğ‘–, ğ‘¤1}, â€¦ ,{ğ‘¤ğ‘–, ğ‘¤ğ‘˜}
  
  * Then given ğ‘¤ğ‘–, predict the occurrence of ğ‘¤ğ‘— using binary classifications:
    â—‹ ğ‘¤ğ‘– co-occurs with ğ‘¤ğ‘—: truth label 1
    â—‹ ğ‘¤ğ‘– does not co-occur with any ğ‘¤ğ‘˜â€² (1 â‰¤ ğ‘˜' â‰¤ ğ‘˜): truth label 0
  
  * ![](./img/3-6-NGS.png)
  
  * ç±»æ¯”ä¸€ä¸‹
    
    - æƒ³è±¡ä½ åœ¨å­¦ä¹ â€œè‹¹æœâ€è¿™ä¸ªè¯çš„è¯­ä¹‰ï¼ˆè¯å‘é‡ï¼‰ã€‚
    - æ­£æ ·æœ¬ï¼šä½ çŸ¥é“â€œè‹¹æœâ€å’Œâ€œæ°´æœâ€ç»å¸¸ä¸€èµ·å‡ºç°ï¼ˆä¸Šä¸‹æ–‡ç›¸å…³ï¼‰ã€‚
    - è´Ÿæ ·æœ¬ï¼šä½ éšæœºæŠ½å–ä¸€äº›ä¸ç›¸å…³çš„è¯ï¼ˆå¦‚â€œæ±½è½¦â€â€œæ¡Œå­â€ï¼‰ï¼Œæ˜ç¡®å®ƒä»¬ä¸â€œè‹¹æœâ€ä¸ç›¸å…³ã€‚
    - é€šè¿‡äºŒåˆ†ç±»ï¼Œæ¨¡å‹å­¦ä¹ åˆ°â€œè‹¹æœâ€ä¸â€œæ°´æœâ€æ›´æ¥è¿‘ï¼Œä¸â€œæ±½è½¦â€â€œæ¡Œå­â€æ›´ç–è¿œã€‚
  
  * å…¬å¼æ¨å¯¼ä¸ªäººè§‰å¾—è€ƒçš„å¯èƒ½æ€§ä¸æ˜¯å¾ˆå¤§ï¼Œå½“ç„¶å­¦éœ¸éšæ„æ¨ã€‚

#### Hierarchical softmax

Huffman tree: the binary tree with minimal external path weight

* Construct a **Huffman tree**, with each leaf node representing a word
  
  * Each **internal node (a cluster of similar words)** of the graph (except the root and the leaves) is associated to a **vector** that the model is going to learn.
  
  * The probability of a word w given a vector ğ‘¤ğ‘–, **ğ‘ƒ(ğ‘¤|ğ‘¤ğ‘–)**, is equal to the probability of **a random walk** starting at the **root** and ending at the leaf node corresponding to **w**.
  
  * Complexity: **O(log(V))**, corresponding to the length of the path.

* Construct a Huffman tree: merge two nodes with the minimum frequencies and consider them together as a single node; repeat until there is only one node

* åº”è¯¥ä¼šè€ƒä¸€é“è®¡ç®—

* å‡è®¾è¯æ±‡è¡¨æœ‰4ä¸ªè¯ï¼š{â€œtheâ€, â€œcatâ€, â€œdogâ€, â€œbirdâ€}ï¼Œè¯é¢‘åˆ†åˆ«ä¸º {100, 50, 30, 20}

* æ ¹
  
     /   \
   the  n1
  
         /  \
       cat  n2
             /  \
           dog  bird

* - ç›®æ ‡è¯ w2â€‹=dogï¼Œä¸Šä¸‹æ–‡è¯ wiâ€‹ã€‚
  
  - è·¯å¾„ï¼šæ ¹ â†’ n1â€‹ï¼ˆå³ï¼‰â†’ n2â€‹ï¼ˆå·¦ï¼‰â†’ dogã€‚
  
  - è·¯å¾„èŠ‚ç‚¹ï¼šn(w2â€‹,1)=æ ¹,n(w2â€‹,2)=n1â€‹,n(w2â€‹,3)=n2â€‹,n(w2â€‹,4)=dogã€‚
  
  - è·¯å¾„é•¿åº¦ï¼šL(w2â€‹)=4ï¼Œæœ‰ L(w2â€‹)âˆ’1=3 æ¡è¾¹ã€‚
  
  - **å‡è®¾**
  
  - ch(n) æ€»æ˜¯å·¦å­èŠ‚ç‚¹ã€‚
  
  - è·¯å¾„æ–¹å‘ï¼š
    
    - ä»æ ¹åˆ° n1â€‹: å³ï¼ˆé ch(n)ï¼Œ[x]=âˆ’1ï¼‰ã€‚
    - ä» n1â€‹ åˆ° n2â€‹: å·¦ï¼ˆæ˜¯ ch(n1â€‹)ï¼Œ[x]=1ï¼‰ã€‚
    - ä» n2â€‹ åˆ° dog: å·¦ï¼ˆæ˜¯ ch(n2â€‹)ï¼Œ[x]=1ï¼‰ã€‚
  
  - #### æ¦‚ç‡è®¡ç®—ï¼š
    
    $P(w2â€‹âˆ£wiâ€‹)=P(n(w2â€‹,1),right)â‹…P(n(w2â€‹,2),left)â‹…P(n(w2â€‹,3),left)$
    
    - ç¬¬1æ­¥ï¼ˆæ ¹ â†’ n1â€‹ï¼Œå³ï¼‰ï¼š $P(n(w2â€‹,1),right)=Ïƒ(âˆ’vn(w2â€‹,1)Tâ€‹vwiâ€‹â€‹)=Ïƒ(âˆ’væ ¹Tâ€‹vwiâ€‹â€‹)$
    
    - ç¬¬2æ­¥ï¼ˆn1â€‹ â†’ n2â€‹ï¼Œå·¦ï¼‰ï¼š $P(n(w2â€‹,2),left)=Ïƒ(vn(w2â€‹,2)Tâ€‹vwiâ€‹â€‹)=Ïƒ(vn1â€‹Tâ€‹vwiâ€‹â€‹)$
    
    - ç¬¬3æ­¥ï¼ˆn2â€‹ â†’ dogï¼Œå·¦ï¼‰ï¼š $P(n(w2â€‹,3),left)=Ïƒ(vn(w2â€‹,3)Tâ€‹vwiâ€‹â€‹)=Ïƒ(vn2â€‹Tâ€‹vwiâ€‹â€‹)$
    
    - æ€»æ¦‚ç‡ï¼š $P(w2â€‹âˆ£wiâ€‹)=Ïƒ(âˆ’væ ¹Tâ€‹vwiâ€‹â€‹)â‹…Ïƒ(vn1â€‹Tâ€‹vwiâ€‹â€‹)â‹…Ïƒ(vn2â€‹Tâ€‹vwiâ€‹â€‹)$
    * æƒ³è±¡ä½ åœ¨è¿·å®«ä¸­æ‰¾ä¸€æœ¬ä¹¦ï¼ˆè¯ ğ‘¤ï¼‰ï¼Œè¿·å®«æ˜¯éœå¤«æ›¼æ ‘ã€‚æ¯æ¬¡åˆ°è¾¾ä¸€ä¸ªè·¯å£ï¼ˆå†…éƒ¨èŠ‚ç‚¹ï¼‰ï¼Œä½ å›ç­”â€œæ˜¯/å¦â€ï¼ˆå·¦æˆ–å³ï¼‰ï¼Œæœ€ç»ˆåˆ°è¾¾ç›®æ ‡ä¹¦ã€‚é«˜é¢‘ä¹¦ï¼ˆå¸¸ç”¨è¯ï¼‰æ”¾åœ¨é è¿‘å…¥å£çš„åœ°æ–¹ï¼Œè·¯å¾„çŸ­ï¼›ç¨€æœ‰ä¹¦æ”¾å¾—è¿œï¼Œè·¯å¾„é•¿ã€‚æ¨¡å‹å­¦ä¹ å¦‚ä½•åœ¨æ¯ä¸ªè·¯å£åšå‡ºæ­£ç¡®é€‰æ‹©ï¼ˆä¼˜åŒ–å‘é‡ï¼‰ï¼Œä½¿åˆ°è¾¾ç›®æ ‡ä¹¦çš„æ¦‚ç‡æœ€å¤§ã€‚
    
    * â— Minimize negative log likelihood âˆ’ log ğ‘ƒ(ğ‘¤|ğ‘¤ğ‘–)
      â— **Update the vectors** of the nodes in the binary tree that are in the path from root to leaf node
      â— **Speed** of this method is determined by the way in which the binary **tree is constructed** and **words are assigned** to leaf nodes
      â— Huffman tree assigns **frequent words shorter** paths in the tree

### Other word embedding methods

#### GloVe

* Global Vectors for Word Representation
  â—‹ Global statistics (LSA) + local context window (word2vec)
  â—‹ Co-occurrence matrix, decreasing weighting: decay ğ‘‹ğ‘–ğ‘—=1/d (distance of word pairs)

* RNNs can be bi-directional

* Stacked RNNs
  â— RNNs can be stacked.
  â— For each input, multiple representations (hidden states) can be learned.

### Contextualized word embeddings (ELMo)

* Problems with (non-contextual) embeddingsï¼š
  
  * å¯¹äºå¤šä¹‰è¯ï¼Œéä¸Šä¸‹æ–‡åµŒå…¥ç”Ÿæˆçš„å•ä¸€å‘é‡æ— æ³•åŒºåˆ†ä¸åŒå«ä¹‰

* Contextualized embeddings:
  
  * ELMo: Deep contextualized word representations
  
  * From **context independent embeddings** to **context dependent embeddings**
  
  * â— In ELMoâ€™s design, the embedding of one word could have multiple possible answers.
    â— The model only gives a certain embedding for one word when this word is given in a sentence.
  
  * **E**mbeddings from **L**anguage **Mo**dels(uses a bi-directional LSTM to pre-train the language model)

* **Key features**
  
  * Replace static embeddings (lexicon lookup) with **context-dependent** embeddings (produced by a deep neural language model), i.e., each tokenâ€™s representation is **a function of the entire input sentence.**
  
  * Computed by a deep **multi-layer, bidirectional** language model.
  
  * Return for each token a (task-dependent) linear combination of its representation across layers.
  
  * Different layers capture different information.

* Architecture: 
  
  * First layer: character level CNN to get context independent embeddings.
  
  * Each layer of this language model network computes a vector representation for each token.
  
  * Freeze the parameters of the language model.
  
  * For each task: train task-dependent softmax weights to combine the layer-wise representations into a single vector for each token jointly with a task-specific model that uses those vectors.

* ![](./img/3-7-ELMO-1.png)

* ![](./img/3-8-ELMO-2.png)

## Lecture 4 Transformers &Pretraining-finetuning

### Attention

* Problems with contextualized word embeddings
  
  * RNNs/LSTMs have long-term dependency problems, where words/tokens are processed sequentially.
    o Information loss
    o Hard to compute in parallel

* Bi-directional RNNs/LSTMs feature fusion and representation ability is weak (compared with transformers).

* Why Attention?
  
  * To reduce the computational **complexity**.
  
  * To **parallelize** the computation.
  
  * Self-attention connects all positions in a sequence with a constant number of operations to solve the **long-term dependency** problem.
    â—‹ Self-attention could yield more **interpretable** models

* What is Attention?
  
  * An analogy to humanâ€™s brain, to pay more attention to more important information.
  
  * Map a **query** and a set of **key-value pairs** to an output, where the query, keys, values and output are all vectors. 
  
  * The output is a **weighted sum of the values**, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key

* Attention in NLP: aligning while translating

* Attention: formal description
  
  * Given a query vector ğ’’, and a set of key-value pairs (all vectors) {(ğ’Œi,vi)}i=1 to L, we first calculate the **similarity/attention score** between the query and each key: ğ‘ i = ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ (ğ’’, ğ’Œi)
  
  * Normalize the similarity score to be between 0 and 1, and they sum up to 1. These are called **attention distribution**. One way is to use the softmax operation.
    ğ‘i = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ i) =exp(si)/Î£iLexp(si) 
  
  * Compute the **attention/context vector** ğ’› as a weighted sum of values.z=Î£iL aivi
  
  * Keys and values are not necessarily the same, and they could be different as well, such as in machine translations.
  
  * ![](./img/4-1-Attention.png)
  
  * ![](./img/4-2-AttentionCaulcation.png)

* Attention is all you need: self-attention
  
  * For an input sequence of words, play attention mechanisms between every word and others (including itself).
  
  * Features
    o Constant path length between any two positions
    o Easy to parallelize per layer
  
  * ![](./img/4-3-SelfAttention.png)
  
  * ![](./img/4-4-SelfAttention-2.png)  
  
  * ![](./img/4-4-SelfAttention-3.png)
  
  * ![](./img/4-4-SelfAttention-4.png)
  
  * How to combine multi-headed output together ?Concatenate them and use another linear transformation.
  
  * ![](./img/4-5-SelfAttention-5.png)
  
  * Graphical view and mathematical presentations
    o Linearly project the queries, keys and values â„ times with different, learned linear projections to ğ‘‘k, ğ‘‘k, ğ‘‘v dimensions respectively.
    o On each of these projected versions, perform the attention function in parallel.
    o The resulting ğ‘‘v - dimensional output values are concatenated and once again projected.
  
  * Parameters: hidden size ğ‘‘model, self-attention heads â„

### Transformer

* Main technique: multi-head self attention mechanism.

* The transformer is a novel architecture that aims to solve sequence-to-sequence tasks while handling long range dependencies with ease.

* ![](./img/4-6-Transformer.png)

* 

### BERT

### GPT
